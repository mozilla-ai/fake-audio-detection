{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Blueprint docs","text":"<p>Blueprints are customizable workflows that help developers build AI applications using open-source tools and models</p> <p>These docs are your companion to mastering the fake audio detection Blueprint.</p>"},{"location":"#built-in-collaboration-with-uncovai-using","title":"Built in collaboration with Uncovai using:","text":"<ul> <li>Python 3.10+</li> <li>Sklearn</li> <li>Librosa</li> </ul>"},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-building-the-blueprint-in-minutes","title":"Start building the Blueprint in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of using the Blueprint</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Tailor project parameters to fit your needs</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p> <p></p>"},{"location":"#why-blueprints","title":"Why Blueprints?","text":"<p>Blueprints are more than starter code\u2014they\u2019re your gateway to building AI-powered solutions with confidence. With step-by-step guidance, modular design, and open-source tools, we make AI accessible for developers of all skill levels.</p>"},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>This Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#changing-the-model","title":"\ud83e\udde0 Changing the Model","text":"<p>You can test other classification models by modifying the code in <code>train_svm.py</code>. Keep the pipeline with the normalizer, as it improves accuracy and generalization.</p> <pre><code>pipeline = Pipeline([\n        ('normalizer', CustomNormalizer(method=normalisation_type)),\n        ('svm', SVC(kernel=kernel, C=C, gamma=gamma,\n                   probability=probability, class_weight=class_weight,\n                   random_state=random_state))\n    ])\n</code></pre> <p>There are many other available models that could work well for audio classification, such as:</p> <ul> <li>Random Forest: Good for handling large datasets with high dimensionality</li> <li>XGBoost: Provides excellent performance for classification tasks with gradient boosting</li> </ul>"},{"location":"customization/#feature-extraction-customization","title":"\ud83c\udfb5 Feature Extraction Customization","text":"<p>The current implementation uses a specific set of audio features. You can enhance or modify the feature extraction process by:</p> <ul> <li>Experimenting with different MFCC configurations</li> </ul> <p>you can add them here</p> <pre><code># extract_features.py\n\ndef extract_features_from_array(y: np.ndarray, sr: float) -&gt; np.ndarray:\n    \"\"\"\n    Extracts audio features from a loaded audio sample.\n\n    Args:\n        y: Audio time series array\n        sr: Sample rate of the audio\n\n    Returns:\n        Array of extracted audio features\n    \"\"\"\n\n    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n    rms = librosa.feature.rms(y=y)\n    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n    imfccs = compute_imfcc(y=y, sr=sr, n_imfcc=13)\n\n    [...]\n</code></pre>"},{"location":"customization/#hyperparameter-tuning","title":"\u2699\ufe0f Hyperparameter Tuning","text":"<p>For better model performance, consider implementing:</p> <ul> <li>Grid search or random search for optimal parameters</li> <li>Cross-validation strategies to prevent overfitting</li> <li>Bayesian optimization for more efficient parameter search</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Our interactive Streamlit demonstration provides a quick way to explore the blueprint's capabilities.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>First, install all required dependencies:</p> <pre><code>pip install .\n</code></pre>"},{"location":"getting-started/#running-the-demo","title":"Running the Demo","text":"<p>Launch the demo application with the provided script:</p> <pre><code># Execute from the fake_audio_detection root directory\n./demo/run.sh\n</code></pre> <p>The application will start automatically and open in your default web browser.</p>"},{"location":"results/","title":"Results: Comparison with other methods &amp; metrics","text":"<p>Below you'll find our results with an SVM model trained on FOR_rerec and FOR_2sec datasets and tested on the InTheWild dataset (available here). We can compare these results to those from the paper MLAAD: The Multi-Language Audio Anti-Spoofing Dataset.</p>"},{"location":"results/#model-performance-comparison-all-models-trained-on-identical-datasets","title":"Model Performance Comparison: All Models Trained on Identical Datasets","text":"Model Accuracy Our SVM 68.9% SLL W2V2 57.8% Whisper DF 54.1% RAWGAT-ST 49.8% <p>Performance Gain: Our SVM model shows an improvement of at least 11% compared to the best model (SLL W2V2) reported in the MLAAD paper.</p> <p>The InTheWild dataset is available here.</p>"},{"location":"results/#detailed-results","title":"detailed results","text":""},{"location":"results/#overall-metrics","title":"\ud83d\udd0d Overall Metrics","text":"Metric Value Accuracy 0.6886 Precision 0.6834 Recall 0.6886 F1 Score 0.6801 ROC AUC 0.7484 Error Rate 0.3114"},{"location":"results/#classification-report","title":"\ud83e\uddfe Classification Report","text":"Class Label Precision Recall F1 Score Support 0 Fake 0.7068 0.8142 0.7568 46,966 1 Real 0.6490 0.5042 0.5675 31,991"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How this Blueprint Works","text":"<p>Here we will describe all the steps we are taking to train an SVM model for fake audio detection.</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>To train an audio classification model in machine learning, you need the following components:</p> <ul> <li>Dataset of audio files (format doesn't matter - it can be mp3, wav, etc.)</li> <li>Audio feature extraction tools to extract important information from the dataset. These same tools will be needed when you want to predict whether an audio file outside the dataset is fake or real.</li> <li>The base model - in our case, an SVM model using an RBF kernel</li> <li>Some patience! Training with SVM can take significant time</li> </ul>"},{"location":"step-by-step-guide/#step-1-download-the-dataset","title":"Step 1: Download the Dataset","text":"<p>First, download the FOR_rerec and FOR_2sec datasets from UncovAI's Huggingface page. These are common datasets used for training models against audio spoofing and deepfakes.</p> <p>Place both datasets under one folder like <code>datasets</code>. The system will check for all sub-datasets in this location and use them for training only if they follow the same format as FOR_rerec and FOR_2sec (audio files organized under FAKE or REAL folders).</p> <p>The location of the file doesn't matter; you will pass its absolute path in the arguments of the AI trainer function <code>train_svm</code> in <code>model.py</code>.</p>"},{"location":"step-by-step-guide/#step-2-create-a-training-script","title":"Step 2: Create a Training Script","text":"<p>Now that we have the dataset, you can create a simple Python file that uses the <code>train_svm</code> function. This will process the referenced dataset, train the model, and save it.</p> <pre><code># audio-detection-trainer.py\n\nfrom fake_audio_detection.model import train_svm\n\nmetrics = train_svm(\"path/to/dataset\", \".\")\n\nprint(\"Classification Metrics:\")\nprint(\"-\" * 30)\nfor metric_name, metric_value in metrics.items():\n    print(f\"{metric_name.capitalize()}: {metric_value:.4f}\")\n</code></pre> <p>To run it do</p> <pre><code>cd src\npython3 -m fake_audio_detection.audio-detection-trainer\n</code></pre>"},{"location":"step-by-step-guide/#step-3-update-the-model-path","title":"Step 3: Update the Model Path","text":"<p>After training, <code>train_svm</code> will print the trained model's location:</p> <pre><code>Model saved to: path/to/my_model.joblib\n</code></pre> <p>Now in <code>demo/app.py</code>, you can modify the following code:</p> <pre><code># app.py\n\nMODEL_PATH = os.path.join(APP_DIR, \"model/noma-1\")\n</code></pre> <p>Replace it with:</p> <pre><code># app.py\n\nMODEL_PATH = \"path/to/my_model.joblib\"\n</code></pre> <p>Congratulations! You can now test your model using Streamlit:</p> <pre><code># from repository root\n./demo/run.sh\n</code></pre>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"}]}